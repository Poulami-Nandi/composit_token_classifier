# ğŸ”¢ Multi-Token Handwritten Recognition: Digits, Letters & Alphanumerics

> ğŸ§  Developed by **Dr. Poulami Nandi** â€” Data Scientist & ML Researcher with a PhD in Quantum Physics from IIT Kanpur and Postdoc from the University of Pennsylvania.

<p align="center">
  <img src="assets/pipeline.png" width="700"/>
</p>

---

## ğŸ—‚ï¸ Project Summary

This project extends classic MNIST digit recognition to a more general task: recognizing sequences of **1 to 10 handwritten tokens** â€“ digits, letters, and alphanumeric characters â€“ placed side by side. It simulates real-world OCR scenarios and explores how classification accuracy degrades with sequence length.

### ğŸ† Key Features

- Supports:
  - **Digit-only tokens (0â€“9)** from MNIST
  - **Letter tokens (Aâ€“Z)** from EMNIST (letters split)
  - **Alphanumeric tokens (Aâ€“Z, aâ€“z, 0â€“9)** from EMNIST (byclass)
- Flexible CNN model with multiple softmax heads â€“ one per token
- Dynamic data generator to create composite sequences of variable length
- In-depth **EDA, training visualization, and error analysis**
- Evaluates **sequence-level accuracy** and per-token metrics
- Modular architecture suitable for curriculum learning or OCR pretraining

---

## ğŸ§  Architecture Overview

<p align="center">
  <img src="assets/loss_curves.png" width="500"/>
</p>

- **Input:** Grayscale images of shape (28 Ã— *n*, 28), where *n* is number of tokens
- **Backbone:** 2 Convolutional + MaxPooling â†’ Dense(128)
- **Heads:** Each token is predicted via an independent `Dense(10|26|62)` layer
- **Loss:** Categorical Crossentropy per head
- **Metrics:** Per-token accuracy, Full-sequence accuracy

---

## ğŸ§© Function Breakdown

### ğŸ”¹ `build_dataset(n_digits=3, mode="digit", sample_limit=50000)`

- Dynamically builds composite sequences from MNIST/EMNIST.
- Tokens are randomly drawn and `np.hstack`ed to simulate sequences.
- Returns `train`, `val`, and `test` splits with token-wise one-hot labels.
- Modes: `digit`, `letter`, `alphanumeric`

### ğŸ”¹ `eda_summary(x, ys, n_digits, title_prefix)`

- Plots:
  - **Pixel intensity distribution** across samples
  - **Class balance** for first token
- Helps detect skewed training data or bias in sampling

### ğŸ”¹ `build_model(n_digits, vocab_size)`

- Builds a Keras model with:
  - Shared CNN feature extractor
  - Multiple softmax heads depending on token count
  - Output shape: `[(None, vocab_size)] * n_digits`
- Supports:
  - `vocab_size = 10` (digits), `26` (letters), `62` (alphanum)

### ğŸ”¹ `train_and_evaluate(n_digits, mode, sample_size)`

- Trains and evaluates the model on sequences of `n_digits` tokens
- Returns:
  - `digit-wise accuracy`
  - `sequence accuracy`
  - training `history`
  - validation `loss/accuracy` plots
  - confusion matrix for top-k predictions

### ğŸ”¹ `compare_performance_across_lengths()`

- Runs `train_and_evaluate` for 1 to 10 token lengths
- Plots sequence accuracy vs. number of tokens
- Generates summary table

---

## ğŸ“Š Exploratory Data Analysis (EDA)

<p align="center">
  <img src="assets/sample_composite.png" width="450"/>
</p>

- Shows how pixel intensity is distributed
- Tracks class imbalance across token types
- Samples shown per token position

---

## ğŸ§ª Results

| Tokens | Sequence Accuracy | Sample Size | Top-1 Token Accuracy |
|--------|-------------------|-------------|----------------------|
| 1-digit   | 0.992           | 10k         | 0.992                |
| 2-digit   | 0.983           | 50k         | 0.993, 0.991         |
| 3-digit   | 0.973           | 60k         | 0.990, 0.988, 0.987  |
| ...       | ...             | ...         | ...                  |
| 10-digit  | ~0.800          | 60k         | ~0.92â€“0.94 range     |

> ğŸ“Œ As the number of tokens increases, **sequence accuracy declines faster** than per-token accuracy due to compounding error.

---

## ğŸ”  Supported Modes

| Mode | Dataset Used | Characters Covered | Output Heads |
|------|--------------|--------------------|---------------|
| `digit` | MNIST        | 0â€“9                | 10            |
| `letter` | EMNIST/letters | Aâ€“Z              | 26            |
| `alphanumeric` | EMNIST/byclass | Aâ€“Z, aâ€“z, 0â€“9 | 62          |

---

## ğŸ“ Directory Structure

```
composite_token_classifier/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ assets/                   â† banners, loss plots, confusion heatmaps
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ digit_letter_alpha_classifier.ipynb
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_utils.py
â”‚   â”œâ”€â”€ model_utils.py
â”‚   â”œâ”€â”€ viz_utils.py
â”‚   â””â”€â”€ train.py
â””â”€â”€ examples/
    â””â”€â”€ quick_demo.py
```

---

## ğŸ“¦ Installation

```bash
git clone https://github.com/Poulami-Nandi/composite_token_classifier.git
cd composite_token_classifier
pip install -r requirements.txt
```

---

## ğŸš€ Try It Yourself

Run 3-digit digit model with 50K samples:
```python
from train import train_and_evaluate
train_and_evaluate(n_digits=3, mode="digit", sample_size=50000)
```

Compare digit vs. letter vs. alphanumeric accuracy:
```python
for mode in ["digit", "letter", "alphanumeric"]:
    for n in range(1, 6):
        train_and_evaluate(n_digits=n, mode=mode, sample_size=60000)
```

---

## ğŸ” Use Cases

- Handwriting OCR with variable-length input
- Document parsing from scanned forms
- Pretraining for language-to-vision token models
- Sequence recognition in low-resource languages

---

## ğŸ‘©â€ğŸ’» About Me

**Dr. Poulami Nandi**  
PhD in Quantum Physics â€“ IIT Kanpur  
Postdoc â€“ University of Pennsylvania  

ğŸ’¼ *Currently a Data Scientist applying ML across Healthcare, Finance & Physics*  
ğŸ”— [LinkedIn](https://www.linkedin.com/in/poulami-nandi-a8a12917b/)  
ğŸ“š [Google Scholar](https://scholar.google.co.in/citations?user=bOYJeAYAAAAJ&hl=en)

---

## ğŸ“œ License

This project is licensed under the [MIT License](LICENSE).

---

## ğŸ™ Acknowledgments

- `tensorflow.keras.datasets.mnist`
- `tensorflow_datasets.emnist`
- Academic inspiration from quantum pattern recognition and digit tokenization

